// File: backend/agentos/core/vision/types/VisionOutput.ts
/**
 * @file VisionOutput.ts
 * @module backend/agentos/core/vision/types/VisionOutput
 * @version 1.0.0
 * @description Defines the structured outputs generated by the vision subsystem after analyzing visual input.
 * This includes comprehensive scene understanding, object detection, OCR results, and other analyses.
 *
 * Key structures:
 * - `VisionTask`: Enum for types of vision analysis tasks.
 * - `BoundingBox`: Represents a rectangular bounding box.
 * - `DetectedObject`: Information about a single object detected in an image.
 * - `SceneUnderstanding`: Holistic description and context of the visual scene.
 * - `OCRWord`, `OCRLine`, `OCRBlock`, `OCRResult`: Text extracted from an image with detailed structure.
 * - `FaceAttributes`, `FaceLandmark`, `FaceDetectionResult`: Information about detected faces.
 * - `ImageFeatureVector`, `ImageCategoricalFeatures`, `ImageFeatureSet`: Abstract representations of image features.
 * - `FrameComparisonResult`: Outcome of comparing two frames.
 * - `ProcessedVisionData`: The primary, consolidated output object containing all analysis results.
 */

/**
 * @enum VisionTask
 * @description Defines the types of vision analysis tasks that can be requested from an IVisionProvider.
 * This enum helps in specifying desired analyses and interpreting results.
 */
export enum VisionTask {
  /** General-purpose description of the entire scene or image. */
  DESCRIBE_SCENE = 'describe_scene',
  /** Detection and localization of multiple objects within the image. */
  DETECT_OBJECTS = 'detect_objects',
  /** Extraction of text from the image using Optical Character Recognition. */
  EXTRACT_TEXT_OCR = 'extract_text_ocr',
  /** Detection and localization of human faces, optionally with attributes. */
  DETECT_FACES = 'detect_faces',
  /** Extraction of abstract image features (e.g., embeddings) for similarity or other ML tasks. */
  EXTRACT_FEATURES = 'extract_features',
  /** Classification of the image into predefined categories. */
  CLASSIFY_IMAGE = 'classify_image',
  /** Detection of specific visual landmarks (e.g., famous buildings, natural formations). */
  DETECT_LANDMARKS = 'detect_landmarks',
  /** Detection of explicit or harmful content. */
  CONTENT_MODERATION = 'content_moderation',
  /** Provides a detailed analysis of image properties such as dominant colors, quality, etc. */
  ANALYZE_IMAGE_PROPERTIES = 'analyze_image_properties',
}

/**
 * @interface BoundingBox
 * @description Represents a rectangular bounding box for an object or region.
 * Coordinates are typically normalized (0.0 to 1.0 relative to image dimensions)
 * or in absolute pixels. The `unit` field clarifies this.
 */
export interface BoundingBox {
  /** @property {number} x - The x-coordinate of the top-left corner of the box. */
  x: number;
  /** @property {number} y - The y-coordinate of the top-left corner of the box. */
  y: number;
  /** @property {number} width - The width of the bounding box. */
  width: number;
  /** @property {number} height - The height of the bounding box. */
  height: number;
  /**
   * @property {'normalized' | 'pixels'} [unit='normalized']
   * @description The unit of the coordinates and dimensions.
   * 'normalized': Values are floats between 0.0 and 1.0, relative to image width/height.
   * 'pixels': Values are absolute pixel counts from the image origin (usually top-left).
   * @default 'normalized'
   */
  unit?: 'normalized' | 'pixels';
}

/**
 * @interface DetectedObject
 * @description Detailed information about a single object detected in an image.
 */
export interface DetectedObject {
  /** @property {string} [objectId] - A unique identifier for this detected instance, if provided by the model. */
  objectId?: string;
  /** @property {string} label - The primary label or name of the detected object (e.g., "cat", "car", "laptop"). */
  label: string;
  /** @property {string[]} [alternativeLabels] - Optional. Other possible labels or parent categories for the object. */
  alternativeLabels?: string[];
  /** @property {number} confidence - The confidence score (typically 0.0 to 1.0) of the detection. */
  confidence: number;
  /** @property {BoundingBox} [boundingBox] - Optional. The bounding box locating the object in the image. */
  boundingBox?: BoundingBox;
  /** @property {Record<string, any>} [attributes] - Optional. Additional attributes or properties of the object (e.g., color, material, state, sub-parts). */
  attributes?: Record<string, any>;
  /** @property {string} [providerModelId] - Optional. ID of the specific model that detected this object. */
  providerModelId?: string;
}

/**
 * @interface SceneUnderstanding
 * @description Provides a holistic description and contextual understanding of the visual scene.
 */
export interface SceneUnderstanding {
  /** @property {string} generalDescription - A concise, human-readable description of the overall scene. */
  generalDescription: string;
  /** @property {string[]} [tags] - Keywords, labels, or tags describing the scene content, categories, or concepts (e.g., "outdoor", "cityscape", "office interior", "abstract art"). */
  tags?: string[];
  /** @property {Array<{ color: { r: number; g: number; b: number; hex?: string; }; score: number; pixelFraction: number; name?: string; }>} [dominantColors] - Analysis of dominant colors. `r,g,b` usually 0-255. */
  dominantColors?: Array<{
    color: { r: number; g: number; b: number; hex?: string; }; // RGB values, optional hex
    score: number; // Confidence or prevalence score
    pixelFraction: number; // Percentage of image pixels this color represents
    name?: string; // Optional common name for the color
  }>;
  /** @property {string} [imageQualityAssessment] - Assessment of image quality (e.g., "clear", "blurry", "low_light", "well-exposed"). */
  imageQualityAssessment?: string;
  /** @property {number} [aestheticScore] - Optional. A score representing the aesthetic quality or appeal of the image (if supported by provider). */
  aestheticScore?: number;
  /** @property {Record<string, any>} [customAnalysis] - Provider-specific or custom scene analysis results (e.g., specific event detection, scene category probabilities). */
  customAnalysis?: Record<string, any>;
}

/**
 * @interface OCRWord
 * @description Represents a single detected word in OCR.
 */
export interface OCRWord {
  text: string;
  confidence: number;
  boundingBox?: BoundingBox;
  languageCode?: string; // Language of this specific word, if identifiable
}

/**
 * @interface OCRLine
 * @description Represents a line of detected text, composed of words.
 */
export interface OCRLine {
  text: string; // Concatenated text of words in the line
  confidence: number; // Average confidence for the line
  boundingBox?: BoundingBox;
  words: OCRWord[];
}

/**
 * @interface OCRBlock
 * @description Represents a block of detected text, composed of lines/paragraphs.
 */
export interface OCRBlock {
  text: string; // Concatenated text of lines in the block
  confidence: number; // Average confidence for the block
  boundingBox?: BoundingBox;
  blockType: 'paragraph' | 'line_group' | 'unknown';
  lines: OCRLine[];
  languageCode?: string; // Predominant language of this block
}

/**
 * @interface OCRResult
 * @description Represents text extracted from an image (Optical Character Recognition).
 */
export interface OCRResult {
  /** @property {string} fullTextAnnotation - The full block of text extracted from the image, potentially with preserved formatting like newlines. */
  fullTextAnnotation: string;
  /** @property {string} [detectedLanguageCode] - Predominant detected language of the text (BCP-47 format). */
  detectedLanguageCode?: string;
  /** @property {OCRBlock[]} [blocks] - Optional. Structured text, broken down into blocks, lines, and words. */
  blocks?: OCRBlock[];
}

/**
 * @interface FaceAttributes
 * @description Common attributes that might be detected for a face.
 * Providers may offer a subset or superset of these. Ethical considerations apply.
 */
export interface FaceAttributes {
  ageRange?: { low: number; high: number };
  gender?: { value: 'male' | 'female' | 'non_binary' | 'unknown'; confidence: number };
  smile?: { value: boolean; confidence: number };
  eyeglasses?: { value: boolean; confidence: number };
  sunglasses?: { value: boolean; confidence: number };
  beard?: { value: boolean; confidence: number };
  mustache?: { value: boolean; confidence: number };
  eyesOpen?: { value: boolean; confidence: number };
  mouthOpen?: { value: boolean; confidence: number };
  emotions?: Array<{ type: string; confidence: number }>; // e.g., { type: 'JOY', confidence: 0.9 }
  headwear?: { value: boolean; confidence: number };
  faceOccluded?: { value: boolean; confidence: number };
  blur?: { value: 'low' | 'medium' | 'high'; confidence: number };
  exposure?: { value: 'underexposed' | 'overexposed' | 'good'; confidence: number };
}

/**
 * @interface FaceLandmark
 * @description A specific point on a detected face.
 */
export interface FaceLandmark {
  type: string; // e.g., 'left_eye', 'right_eye_pupil', 'nose_tip', 'mouth_center'
  x: number;
  y: number;
  z?: number; // Optional depth coordinate
  confidence?: number;
}

/**
 * @interface FaceDetectionResult
 * @description Information about a detected human face.
 * Note: Face recognition (identifying specific individuals) is typically a separate, more sensitive capability
 * and is NOT included here. This focuses on detection and attribute analysis.
 */
export interface FaceDetectionResult {
  /** @property {string} [faceId] - A unique identifier for this detected face instance, if provided by the model. */
  faceId?: string;
  /** @property {BoundingBox} boundingBox - The bounding box of the detected face. */
  boundingBox: BoundingBox;
  /** @property {number} confidence - Confidence score for the face detection itself (0.0 to 1.0). */
  confidence: number;
  /** @property {FaceLandmark[]} [landmarks] - Optional. An array of detected facial landmarks. */
  landmarks?: FaceLandmark[];
  /** @property {{ pitch: number; roll: number; yaw: number; confidence?: number }} [pose] - Optional. Head pose angles in degrees. */
  pose?: { pitch: number; roll: number; yaw: number; confidence?: number };
  /** @property {FaceAttributes} [attributes] - Optional. Detected attributes of the face. */
  attributes?: FaceAttributes;
  /** @property {number} [detectionQualityScore] - Optional. A score indicating the quality of the detection (e.g., how clearly the face is visible). */
  detectionQualityScore?: number;
}

/**
 * @interface ImageFeatureVector
 * @description Represents a numerical feature vector (embedding) extracted from an image.
 * This is typically used for similarity comparisons or as input to other machine learning models.
 */
export interface ImageFeatureVector {
  /** @property {'embedding_vector'} type - Discriminator for the union type. */
  type: 'embedding_vector';
  /** @property {number[]} vector - The numerical vector representing the image features. */
  vector: number[];
  /** @property {string} [modelId] - ID of the model that generated this feature vector. */
  modelId?: string;
  /** @property {number} [dimensions] - The dimensionality of the vector. */
  dimensions?: number;
  /** @property {string} [normalizationUsed] - Information about any normalization applied to the vector (e.g., "l2_norm", "unit_length"). */
  normalizationUsed?: string;
}

/**
 * @interface ImageCategoricalFeatures
 * @description Represents categorical or textual features extracted from an image,
 * such as descriptive tags or dominant object types.
 */
export interface ImageCategoricalFeatures {
  /** @property {'categorical_features'} type - Discriminator for the union type. */
  type: 'categorical_features';
  /** @property {Array<{label: string, confidence?: number}>} [tags] - Descriptive tags or labels with optional confidence scores. */
  tags?: Array<{label: string, confidence?: number}>;
  /** @property {string} [dominantObjectType] - Primary type of object or scene category identified. */
  dominantObjectType?: string;
  /** @property {Record<string, string | number>} [customAttributes] - Other descriptive attributes. */
  customAttributes?: Record<string, string | number>;
  /** @property {string} [modelId] - ID of the model that generated these categorical features. */
  modelId?: string;
}

/**
 * @interface ImageDigestFeatures
 * @description Represents features based on image digests or hashes.
 */
export interface ImageDigestFeatures {
    /** @property {'digest_features'} type - Discriminator for the union type. */
    type: 'digest_features';
    /** @property {string} [md5] - MD5 hash of the image. */
    md5?: string;
    /** @property {string} [sha256] - SHA256 hash of the image. */
    sha256?: string;
    /** @property {string} [perceptualHash] - Perceptual hash (e.g., pHash, aHash, dHash) string. */
    perceptualHash?: string;
    /** @property {string} [perceptualHashAlgorithm] - Algorithm used for `perceptualHash`. */
    perceptualHashAlgorithm?: string;
}


/**
 * @union ImageFeatureSet
 * @description A union type representing different kinds of image features that can be extracted.
 * This allows flexibility for various feature types from different models or for different purposes.
 */
export type ImageFeatureSet =
  | ImageFeatureVector
  | ImageCategoricalFeatures
  | ImageDigestFeatures
  | { type: 'custom'; value: any; modelId?: string }; // For provider-specific complex feature objects

/**
 * @interface FrameComparisonResult
 * @description The result of comparing two image frames or their features.
 */
export interface FrameComparisonResult {
  /** @property {number} similarityScore - A score indicating similarity (e.g., 0.0 to 1.0, where 1.0 is highly similar/identical). Interpretation depends on the method. */
  similarityScore: number;
  /** @property {number} [differenceScore] - A score indicating difference (often `1 - similarityScore`, or a specific metric like Hamming distance). */
  differenceScore?: number;
  /** @property {string} comparisonMethod - The method used for comparison (e.g., "feature_cosine_similarity", "phash_hamming_distance", "pixel_mse"). */
  comparisonMethod: string;
  /** @property {boolean} [isSignificantChange] - Optional flag derived by applying a threshold to the difference/similarity score. */
  isSignificantChange?: boolean;
  /** @property {Record<string, any>} [details] - Additional details about the comparison (e.g., specific differing regions, feature distances). */
  details?: Record<string, any>;
}

/**
 * @interface ProcessedVisionData
 * @description The primary, consolidated output object from the vision subsystem after analyzing an image.
 * It aggregates results from various analysis tasks performed by an `IVisionProvider`.
 */
export interface ProcessedVisionData {
  /** @property {string} inputId - An identifier for the input that was processed (e.g., derived from `VisionInputEnvelope.envelopeId` or `FrameMetadata.frameId`). */
  inputId: string;
  /** @property {number} processingTimestamp - Unix epoch (ms) when this analysis was completed by the provider. */
  processingTimestamp: number;
  /** @property {string} visionProviderId - ID of the `IVisionProvider` that performed the analysis. */
  readonly visionProviderId: string;
  /** @property {string} modelIdUsed - ID of the specific model used by the provider for this analysis. */
  readonly modelIdUsed: string;

  /** @property {SceneUnderstanding} [sceneUnderstanding] - Results of scene description and analysis. Populated if `VisionTask.DESCRIBE_SCENE` or `VisionTask.ANALYZE_IMAGE_PROPERTIES` was requested. */
  sceneUnderstanding?: SceneUnderstanding;
  /** @property {DetectedObject[]} [detectedObjects] - Array of objects detected in the image. Populated if `VisionTask.DETECT_OBJECTS` was requested. */
  detectedObjects?: DetectedObject[];
  /** @property {OCRResult} [ocrResult] - Text extracted from the image. Populated if `VisionTask.EXTRACT_TEXT_OCR` was requested. */
  ocrResult?: OCRResult;
  /** @property {FaceDetectionResult[]} [faceDetections] - Array of detected faces. Populated if `VisionTask.DETECT_FACES` was requested. */
  faceDetections?: FaceDetectionResult[];
  /** @property {ImageFeatureSet} [imageFeatures] - Extracted abstract features of the image. Populated if `VisionTask.EXTRACT_FEATURES` was requested. */
  imageFeatures?: ImageFeatureSet;
  /** @property {Array<{label: string, confidence: number}>} [imageClassifications] - Classification results. Populated if `VisionTask.CLASSIFY_IMAGE` was requested. */
  imageClassifications?: Array<{label: string, confidence: number}>;
  /** @property {any} [contentModerationResult] - Moderation results. Structure depends on provider. Populated if `VisionTask.CONTENT_MODERATION` was requested. */
  contentModerationResult?: any; // Define a more specific interface if common structure emerges

  /** @property {Record<string, any>} [rawProviderResponse] - Optional. The raw, unprocessed response from the underlying vision service. Useful for debugging or accessing provider-specific details not mapped to the standard structure. */
  rawProviderResponse?: Record<string, any>;
  /** @property {string[]} [errors] - Optional. An array of error messages encountered during processing specific to this image by the provider. General provider errors would throw a `VisionError`. */
  errors?: string[];
  /** @property {number} [processingTimeMs] - Optional. Time taken by the provider (API call duration) to process this input, in milliseconds. */
  processingTimeMs?: number;
  /** @property {VisionTask[]} completedTasks - List of tasks that were successfully completed for this image. */
  completedTasks: VisionTask[];
}