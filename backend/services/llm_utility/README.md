# AgentOS: Utility LLM Service

**Version:** 1.0  
**Status:** Core Implementation

## 1. Overview üìù

The UtilityLLMService provides a set of general-purpose functionalities leveraging Large Language Models (LLMs) through the configured AIModelProviderManager and IPromptEngine. Unlike the complex, persona-driven GMI (Generalized Mind Instance), this service offers more direct access to common LLM tasks such as direct prompting and text summarization.

It is designed to be:

- **Provider-Agnostic**: Works with any LLM provider configured in AIModelProviderManager.
- **Context-Aware Prompting**: Utilizes an IPromptEngine for sophisticated prompt construction, adapting to the model and task.
- **Cost-Aware**: LLM calls made through this service return usage and estimated cost information, inherited from the underlying provider layer.
- **Configurable**: Allows specifying models, providers, and various completion options on a per-request basis.
- **Stream-Capable**: Supports streaming responses for applicable methods, providing real-time output.

## 2. Core Functionalities & API üõ†Ô∏è

The service is typically instantiated once with necessary dependencies and then shared or injected where needed.

```typescript
import { AIModelProviderManager } from './backend/agentos/core/llm/providers/AIModelProviderManager';
import { IPromptEngine } from './backend/agentos/core/llm/IPromptEngine';
import { UtilityLLMService } from './backend/services/llm_utility/UtilityLLMService';

// Assuming providerManager and promptEngine are already initialized instances
// const providerManager = new AIModelProviderManager();
// await providerManager.initialize(managerConfig);
// const promptEngine = new PromptEngine();
// await promptEngine.initialize(promptEngineConfig);

// const utilityLLMService = new UtilityLLMService(providerManager, promptEngine);
```

### 2.1. Direct Prompting

This functionality allows for sending a prompt, along with an optional system message, directly to an LLM and receiving its generated response. It's suitable for a wide range of ad-hoc LLM interactions.

- **`processDirectPrompt(request: DirectPromptRequest): Promise<UtilityLLMServiceOutput>`**
  - Sends a request and returns a consolidated response once the LLM finishes processing.
- **`streamDirectPrompt(request: DirectPromptRequest): AsyncIterable<UtilityLLMServiceOutput>`**
  - Sends a request and returns an asynchronous iterable that yields response chunks as they are generated by the LLM.

#### DirectPromptRequest Interface:
| Field | Type | Description |
|-------|------|-------------|
| prompt | string | Required. The main user prompt or query. |
| systemPrompt | string (optional) | System-level instructions to guide the LLM's behavior. |
| modelId | string (optional) | Specific model ID (e.g., "gpt-4o-mini", "ollama/llama3"). |
| providerId | string (optional) | Specific provider ID (e.g., "openai", "ollama"). |
| taskHint | string (optional) | A hint for the task (e.g., "code_generation", "creative_writing") to aid model selection. |
| completionOptions | Partial<ModelCompletionOptions> (opt) | Fine-tuning parameters for the LLM (see ModelCompletionOptions below). |
| stream | boolean (optional) | Set to true for streamDirectPrompt. If true for processDirectPrompt, it implies internal handling. |

#### UtilityLLMServiceOutput Interface (for both methods):
| Field | Type | Description |
|-------|------|-------------|
| responseText | `string \| null` | The generated text response from the LLM. |
| usage | ModelUsage (opt) | Token counts (prompt, completion, total) and estimated costUSD. Typically present in the final chunk for streams. |
| error | string (optional) | An error message if the operation failed. |
| isFinal | boolean (optional) | true if this is the final output/chunk, false otherwise. For processDirectPrompt, this is always true in the returned Promise. |
| providerId | string (optional) | The ID of the LLM provider used. |
| modelIdUsed | string (optional) | The ID of the LLM model used. |

#### ModelCompletionOptions (Partial, from IProvider.ts):
Common options include temperature, maxTokens, topP, presencePenalty, frequencyPenalty, stopSequences, tools, toolChoice, responseFormat.

### 2.2. Text Summarization

This functionality generates a concise summary of a given piece of text, with options to control the desired length and output format.

- **`summarizeText(request: SummarizationTaskRequest): Promise<UtilityLLMServiceOutput>`**
  - Generates and returns a complete summary. If request.stream is true, it will internally aggregate the streamed response.
- **`streamSummarizeText(request: SummarizationTaskRequest): AsyncIterable<UtilityLLMServiceOutput>`**
  - Generates a summary and streams it chunk by chunk.

#### SummarizationTaskRequest Interface:
| Field | Type | Description |
|-------|------|-------------|
| textToSummarize | string | Required. The input text to be summarized. |
| desiredLength | `'short' \| 'medium' \| 'long'` (opt) | Desired length of the summary. |
| outputFormat | `'paragraph' \| 'bullet_points'` (opt) | Format of the summary output. |
| modelId | string (optional) | Specific model ID to use for summarization. |
| providerId | string (optional) | Specific provider ID. |
| completionOptions | Partial<ModelCompletionOptions> (opt) | Additional LLM completion parameters. |
| stream | boolean (optional) | true to request a streaming summary (primarily for streamSummarizeText or internal handling). |

(Output is UtilityLLMServiceOutput, similar to Direct Prompting.)

## 3. Model Selection Logic üß†

The service employs an internal `_selectProviderAndModel` method to determine the LLM provider and model for each request if not explicitly specified. The selection process is as follows:

1. **Explicit IDs**: If both providerId and modelId are provided in the request, they are used directly.
2. **Model ID Only**: If only modelId is given, the service queries AIModelProviderManager to find a suitable provider for that model. This supports prefixed model IDs (e.g., "openai/gpt-4o-mini").
3. **Provider ID Only**: If only providerId is given, the service uses that provider and attempts to select a default or suitable model from that provider based on the taskHint or the provider's default model.
4. **No IDs (Heuristic)**: If neither providerId nor modelId is specified, the service uses a basic heuristic based on the taskHint to select a model from all available models across configured providers. If no specific model is matched, it falls back to the system's default provider and its default model, or ultimately the first available model.

**Note:** For sophisticated, context-aware, or LLM-driven model routing strategies, it's recommended to use the main GMI or a dedicated IModelRouter component before interacting with this service if more advanced model selection intelligence is required.

## 4. Error Handling & Cost Management üí∏

- **Error Reporting**: All service methods return or yield UtilityLLMServiceOutput. If an error occurs (e.g., during prompt construction, LLM API call, or response processing), the error field in the output will be populated with a descriptive message. Errors are wrapped in GMIError for standardized error handling within AgentOS.
- **Cost Transparency**: For successful LLM interactions, the usage field in UtilityLLMServiceOutput provides detailed token metrics (promptTokens, completionTokens, totalTokens) and an estimated costUSD, offering clear insight into resource consumption.

## 5. Usage Example üíª

```typescript
// Assuming 'utilityLLMService' is an initialized instance of UtilityLLMService.

async function runUtilityTasks() {
  try {
    // Example 1: Direct Prompting (Non-Streaming)
    const directPromptOutput = await utilityLLMService.processDirectPrompt({
      prompt: "Translate 'Hello, world!' into French.",
      systemPrompt: "You are a helpful translation assistant.",
      modelId: "ollama/llama3", // Example: Using a local Ollama model
      completionOptions: {
        temperature: 0.7,
        maxTokens: 50,
      },
    });

    if (directPromptOutput.responseText) {
      console.log("Translation:", directPromptOutput.responseText);
      console.log("Direct Prompt Cost:", directPromptOutput.usage?.costUSD?.toFixed(6) || 'N/A');
    } else {
      console.error("Direct Prompt Error:", directPromptOutput.error);
    }

    console.log("\n---\n");

    // Example 2: Text Summarization (Streaming)
    const textForSummary = "AgentOS is a comprehensive framework designed to facilitate the development, deployment, and management of advanced AI agents. It provides robust components for cognitive architecture, memory systems, tool usage, and interaction orchestration. The UtilityLLMService is one such component offering direct access to LLM functionalities for various tasks.";
    
    console.log("Streaming Summary for:", `"${textForSummary.substring(0, 50)}..."\n`);
    let fullSummary = "";
    for await (const chunk of utilityLLMService.streamSummarizeText({
      textToSummarize: textForSummary,
      desiredLength: 'short',
      outputFormat: 'paragraph',
      // modelId: "openai/gpt-4o-mini" // Optionally specify model
    })) {
      if (chunk.responseText) {
        process.stdout.write(chunk.responseText); // Print delta as it arrives
        fullSummary += chunk.responseText;
      }
      if (chunk.isFinal) {
        process.stdout.write("\n"); // Newline after full summary
        console.log("\nFull Streamed Summary:", fullSummary);
        console.log("Summarization Stream Cost:", chunk.usage?.costUSD?.toFixed(6) || 'N/A');
      }
      if (chunk.error) {
        console.error("\nSummarization Stream Error:", chunk.error);
        break; 
      }
    }

  } catch (e: any) {
    // Catch errors from the service call itself (e.g., if _selectProviderAndModel fails)
    console.error("Service Invocation Error:", e instanceof GMIError ? e.toJSON() : e.message);
  }
}

// runUtilityTasks();
```

This UtilityLLMService acts as a foundational element within AgentOS, enabling other system components, including GMIs, to perform essential LLM-based tasks efficiently and directly, without the overhead of full persona management when it's not required.