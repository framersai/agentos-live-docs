/**
 * @file Defines interfaces for Large Language Model (LLM) services and configurations.
 * @description This file specifies the contracts for interacting with different LLM providers,
 * managing their configurations, and handling responses in a standardized way.
 * It supports the architectural goal of provider flexibility (e.g., OpenAI, OpenRouter, Anthropic, Ollama).
 * @version 1.0.1 - Clarified comments and optional properties.
 */

import { LlmProviderId } from "./llm.config.service"; // If LlmProviderId enum is to be used here

/**
 * Represents a single message in a chat conversation.
 * Aligns with common LLM API structures (e.g., OpenAI).
 */
export interface IChatMessage {
  /**
   * The role of the message sender.
   * 'system': Instructions or context for the AI.
   * 'user': Input from the end-user.
   * 'assistant': Responses generated by the AI.
   */
  role: 'system' | 'user' | 'assistant';

  /** The text content of the message. */
  content: string;

  /**
   * Optional name of the participant.
   * Useful for multi-turn conversations or specifying function names in function calling scenarios.
   */
  name?: string;
  // For future OpenAI-compatible tool/function calling:
  // tool_calls?: { id: string; type: 'function'; function: { name: string; arguments: string; }; }[];
  // tool_call_id?: string;
}

/**
 * Represents the token usage statistics returned by an LLM API call.
 */
export interface ILlmUsage {
  /** Number of tokens in the input prompt. Nullable if not provided by the API. */
  prompt_tokens: number | null;
  /** Number of tokens in the generated completion. Nullable if not provided by the API. */
  completion_tokens: number | null;
  /** Total tokens used (prompt + completion). Nullable if not provided by the API. */
  total_tokens: number | null;
}

/**
 * Represents a standardized response from an LLM service.
 */
export interface ILlmResponse {
  /** The primary text content of the AI's response. Null if the response is a tool call or error. */
  text: string | null;
  /** Identifier of the model that generated the response (e.g., "openai/gpt-4o-mini", "anthropic/claude-3-opus"). */
  model: string;
  /** Optional token usage statistics for the API call. */
  usage?: ILlmUsage;
  /** Optional unique identifier for the request or response chunk (if streaming), provided by the LLM. */
  id?: string;
  /**
   * Optional reason why the LLM stopped generating tokens.
   * Common values: "stop" (natural end), "length" (max_tokens limit), "tool_calls", "content_filter".
   */
  stopReason?: string;
  /**
   * Any additional metadata or the raw, unparsed response from the provider.
   * Typed as 'unknown' for safety, requiring explicit type assertion if accessed.
   */
  providerResponse?: unknown;
}

/**
 * Configuration options for an LLM provider.
 */
export interface ILlmProviderConfig {
  /** Unique identifier for the provider (e.g., 'openai', 'openrouter', 'anthropic', 'ollama'). */
  providerId: LlmProviderId | string; // Using LlmProviderId enum or string for flexibility
  /** API key for the provider. Undefined for providers that don't require one (e.g., local Ollama). */
  apiKey: string | undefined;
  /** Base URL for the provider's API. */
  baseUrl?: string;
  /** Default model ID to use for this provider if not specified in a request. */
  defaultModel?: string;
  /** Optional additional HTTP headers to send with requests to this provider (e.g., for OpenRouter site identification). */
  additionalHeaders?: Record<string, string>;
}

/**
 * Common parameters for making a chat completion request to an LLM.
 */
export interface IChatCompletionParams {
  /**
   * Controls randomness. Lower values (e.g., 0.2) make output more focused and deterministic.
   * Higher values (e.g., 0.8) make it more random. Typically between 0.0 and 2.0.
   */
  temperature?: number;
  /** The maximum number of tokens to generate in the response. */
  max_tokens?: number;
  /**
   * Nucleus sampling parameter. The model considers tokens with top_p probability mass.
   * 0.1 means top 10% probability mass. Typically between 0.0 and 1.0.
   * OpenAI recommends altering only one of temperature or top_p.
   */
  top_p?: number;
  /** One or more sequences where the API will stop generating further tokens. */
  stop?: string | string[];
  /** A unique identifier for the end-user, aiding in monitoring and abuse detection. */
  user?: string;
  /**
   * If true, the response will be streamed. Handling streamed responses requires specific client-side logic.
   * This interface and associated services currently focus on non-streamed completions unless specified otherwise by a service.
   */
  stream?: boolean;
  /**
   * Number of chat completion choices to generate for each input message. Defaults to 1.
   */
  n?: number;
  /**
   * Positive values penalize new tokens based on whether they appear in the text so far,
   * increasing the model's likelihood to talk about new topics. Between -2.0 and 2.0.
   */
  presence_penalty?: number;
  /**
   * Positive values penalize new tokens based on their existing frequency in the text so far,
   * decreasing the model's likelihood to repeat the same line verbatim. Between -2.0 and 2.0.
   */
  frequency_penalty?: number;
  // Allow any other provider-specific parameters via index signature
  [key: string]: any;
}

/**
 * Interface for an LLM service.
 * Each concrete implementation (e.g., OpenAiLlmService, OpenRouterLlmService)
 * must adhere to this contract for standardized interaction.
 */
export interface ILlmService {
  /** The unique identifier of the LLM provider this service interacts with (e.g., "openai", "openrouter"). */
  readonly providerId: LlmProviderId | string;

  /**
   * Generates a chat completion based on the provided messages and model.
   * @param {IChatMessage[]} messages - An array of IChatMessage objects representing the conversation history and current prompt.
   * @param {string} modelId - The identifier of the LLM to use (e.g., "gpt-4o-mini", "anthropic/claude-3-opus").
   * @param {IChatCompletionParams} [params] - Optional parameters to customize the completion (e.g., temperature, max_tokens).
   * @returns {Promise<ILlmResponse>} A Promise resolving to a standardized ILlmResponse.
   * @throws {Error} If the API call fails, configuration is missing, or the response cannot be processed.
   */
  generateChatCompletion(
    messages: IChatMessage[],
    modelId: string,
    params?: IChatCompletionParams
  ): Promise<ILlmResponse>;

  // Future methods could include:
  // generateEmbedding(texts: string[], modelId: string): Promise<{ embedding: number[]; usage: ILlmUsage }[]>;
  // listAvailableModels(): Promise<{ id: string; description?: string; contextWindow?: number }[]>;
}